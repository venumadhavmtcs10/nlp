{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYlIDWdGjeOh",
        "outputId": "94343256-6dff-4efc-f1f9-f1347caf4b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n",
            "Total tokens: 17005207\n",
            "Vocabulary size: 71290\n",
            "Filtered tokens: 16718844\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "\n",
        "import os, random, urllib.request\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "url = \"https://mattmahoney.net/dc/text8.zip\"\n",
        "if not os.path.exists(\"text8.zip\"):\n",
        "    urllib.request.urlretrieve(url, \"text8.zip\")\n",
        "\n",
        "!unzip -o text8.zip\n",
        "\n",
        "words = open(\"text8\").read().split()\n",
        "print(\"Total tokens:\", len(words))\n",
        "\n",
        "min_count = 5\n",
        "freq = {}\n",
        "for w in words:\n",
        "    freq[w] = freq.get(w, 0) + 1\n",
        "\n",
        "vocab = {w:c for w,c in freq.items() if c >= min_count}\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = list(vocab)\n",
        "V = len(vocab)\n",
        "\n",
        "tokens = [word2idx[w] for w in words if w in vocab]\n",
        "print(\"Vocab size:\", V)\n",
        "print(\"Filtered tokens:\", len(tokens))\n",
        "\n",
        "window = 2\n",
        "pairs = []\n",
        "\n",
        "for i in range(len(tokens)):\n",
        "    for j in range(max(0, i-window), min(len(tokens), i+window+1)):\n",
        "        if i != j:\n",
        "            pairs.append((tokens[i], tokens[j]))\n",
        "\n",
        "print(\"Training pairs:\", len(pairs))\n",
        "\n",
        "dim = 100\n",
        "neg_k = 5\n",
        "\n",
        "W = nn.Embedding(V, dim)\n",
        "C = nn.Embedding(V, dim)\n",
        "\n",
        "optimizer = optim.Adam(list(W.parameters()) + list(C.parameters()), lr=0.01)\n",
        "\n",
        "epochs = 2\n",
        "max_steps = 2_000_000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    random.shuffle(pairs)\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for t, c in pairs[:max_steps]:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        vt = W(torch.tensor(t))\n",
        "        vc = C(torch.tensor(c))\n",
        "\n",
        "        # positive loss\n",
        "        pos_loss = torch.log(torch.sigmoid(torch.dot(vt, vc)))\n",
        "\n",
        "        # negative sampling (vectorized)\n",
        "        neg_ids = torch.randint(0, V, (neg_k,))\n",
        "        neg_vecs = C(neg_ids)\n",
        "\n",
        "        neg_loss = torch.log(torch.sigmoid(-torch.matmul(neg_vecs, vt))).sum()\n",
        "\n",
        "        loss = -(pos_loss + neg_loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.2f}\")\n",
        "\n",
        "my_vectors = W.weight.detach().numpy()\n",
        "\n",
        "# GENSIM\n",
        "sentences = [tokens[i:i+1000] for i in range(0, len(tokens), 1000)]\n",
        "gensim_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=100,\n",
        "    window=2,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "# COSINE SIMILARITY\n",
        "def cosine(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "print(\"\\nCOSINE SIMILARITY (OURS vs GENSIM)\")\n",
        "for w in [\"king\", \"queen\", \"man\", \"woman\"]:\n",
        "    if w in word2idx and w in gensim_model.wv:\n",
        "        print(w, cosine(my_vectors[word2idx[w]], gensim_model.wv[w]))\n",
        "\n",
        "def analogy(a, b, c, model, topn=5):\n",
        "    target = model[b] - model[a] + model[c]\n",
        "    scores = [(w, cosine(target, model[w])) for w in model.key_to_index]\n",
        "    return sorted(scores, key=lambda x: x[1], reverse=True)[:topn]\n",
        "\n",
        "print(\"\\nANALOGY: king - man + woman\")\n",
        "print(analogy(\"man\", \"king\", \"woman\", gensim_model.wv))\n",
        "\n",
        "# BIAS DETECTION\n",
        "gender_dir = gensim_model.wv[\"man\"] - gensim_model.wv[\"woman\"]\n",
        "\n",
        "def bias(word):\n",
        "    return cosine(gender_dir, gensim_model.wv[word])\n",
        "\n",
        "print(\"\\nGENDER BIAS\")\n",
        "for w in [\"programmer\", \"doctor\", \"nurse\", \"homemaker\"]:\n",
        "    if w in gensim_model.wv:\n",
        "        print(w, bias(w))\n"
      ]
    }
  ]
}